{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Lab 4A: CNN for Text Classification\n",
    "\n",
    "In this Code Lab, we are going to implement *Convolutional Neural Networks for Sentence Classification* (Yoon Kim, 2014).\n",
    "\n",
    "In his [paper](https://arxiv.org/abs/1408.5882), Yoon Kim proposed several techniques to achieve good text classification accuracy with minimal hyper-parameter tuning.\n",
    "\n",
    "This notebook consist of 4 main sections:\n",
    "\n",
    "1. Preparing the data\n",
    "2. Implementing Yoon Kim's CNN model\n",
    "3. Training the model\n",
    "4. Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 100000    # max no. of words for tokenizer\n",
    "MAX_SEQUENCE_LENGTH = 100 # max length of each entry (sentence), including padding\n",
    "VALIDATION_SPLIT = 0.2   # data for validation (not used in training)\n",
    "EMBEDDING_DIM = 100      # embedding dimensions for word vectors (word2vec/GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re, sys, csv, pickle\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from keras import regularizers, initializers, optimizers, callbacks\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare the data\n",
    "**Read from dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pre-trained GloVe vectors\n",
    "\n",
    "import keras\n",
    "\n",
    "GLOVE_URL = \"https://s3-ap-southeast-1.amazonaws.com/deeplearning-iap-material/glove.6B.100d.txt.zip\"\n",
    "GLOVE_DIR = keras.utils.get_file(\"glove.6B.100d.txt.zip\", GLOVE_URL, cache_subdir='datasets', extract=True)\n",
    "print(\"GloVe data present at\", GLOVE_DIR)\n",
    "GLOVE_DIR = GLOVE_DIR.replace(\".zip\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def clean_text(text, remove_stopwords=False):\n",
    "    output = \"\"\n",
    "    text = str(text).replace(\"\\n\", \"\")\n",
    "    text = re.sub(r'[^\\w\\s]','',text).lower()\n",
    "    if remove_stopwords:\n",
    "        text = text.split(\" \")\n",
    "        for word in text:\n",
    "            if word not in stopwords.words(\"english\"):\n",
    "                output = output + \" \" + word\n",
    "    else:\n",
    "        output = text\n",
    "    return str(output.strip())[1:-3].replace(\"  \", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels = [], [] # empty lists for the sentences and labels\n",
    "\n",
    "data_neg = open(\"datasets/stanford_movie_neg.txt\", \"rb\") \n",
    "for line in tqdm_notebook(data_neg, total=5331): \n",
    "    texts.append(clean_text(line, remove_stopwords=False))\n",
    "    labels.append(int(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = open(\"datasets/stanford_movie_pos.txt\", \"rb\") \n",
    "for line in tqdm_notebook(data_pos, total=5331): \n",
    "    texts.append(clean_text(line, remove_stopwords=False))\n",
    "    labels.append(int(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample negative:\", texts[0], labels[0])\n",
    "print(\"Sample positive:\", texts[-1], labels[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_TOKENIZER = True\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "if CACHE_TOKENIZER:\n",
    "    with open('tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"[i] Saved word tokenizer to file: tokenizer.pickle\")\n",
    "\n",
    "# to use cached tokenizer:\n",
    "\"\"\"\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate the array of sequences from dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('[i] Vocabulary size:', len(word_index))\n",
    "\n",
    "# pad on both ends\n",
    "data_int = pad_sequences(sequences, padding='pre', maxlen=(MAX_SEQUENCE_LENGTH-5))\n",
    "data = pad_sequences(data_int, padding='post', maxlen=(MAX_SEQUENCE_LENGTH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the train-validation split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = to_categorical(np.asarray(labels)) # convert the category label to one-hot encoding\n",
    "print('[i] Shape of data tensor:', data.shape)\n",
    "print('[i] Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('[i] Number of entries in each category:')\n",
    "print(\"[+] Training:\",y_train.sum(axis=0))\n",
    "print(\"[+] Validation:\",y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does the data look like?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tokenized sequence:\\n\", data[0])\n",
    "print(\"\")\n",
    "print(\"One-hot label:\\n\", labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create the model\n",
    "Yoon Kim's model has several notable features:\n",
    "![model-structure](notebook_imgs/yoon_kim_structure.png)\n",
    "* two sets of word embeddings for what he terms a **\"multi-channel\" approach**.\n",
    "  * One of the word embeddings will be frozen (**\"static channel\"**), and one will be modified during the training process (**\"non-static channel\"**). \n",
    "* multiple convolutional kernel sizes\n",
    "\n",
    "We will now start to create the model in `Keras`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load word embeddings into an `embeddings_index`**\n",
    "\n",
    "Create an index of words mapped to known embeddings, by parsing the data dump of pre-trained embeddings.\n",
    "\n",
    "We use a set from [pre-trained GloVe vectors from Stanford](https://nlp.stanford.edu/projects/glove/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open(GLOVE_DIR)\n",
    "print(\"[i] (long) Loading GloVe from:\",GLOVE_DIR,\"...\",end=\"\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    embeddings_index[word] = np.asarray(values[1:], dtype='float32')\n",
    "f.close()\n",
    "print(\"Done.\\n[+] Proceeding with Embedding Matrix...\", end=\"\")\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(\" Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second embedding matrix for non-static channel\n",
    "embedding_matrix_ns = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix_ns[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the `Embedding` layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32') # input to the model\n",
    "\n",
    "# static channel\n",
    "embedding_layer_frozen = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "embedded_sequences_frozen = embedding_layer_frozen(sequence_input)\n",
    "\n",
    "# non-static channel\n",
    "embedding_layer_train = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix_ns],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "embedded_sequences_train = embedding_layer_train(sequence_input)\n",
    "\n",
    "l_embed = Concatenate(axis=1)([embedded_sequences_frozen, embedded_sequences_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the CNN layer with multiple kernel (filter) sizes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_conv_3 = Conv1D(filters=64,kernel_size=3,activation='relu')(l_embed)\n",
    "l_conv_4 = Conv1D(filters=64,kernel_size=4,activation='relu')(l_embed)\n",
    "l_conv_5 = Conv1D(filters=64,kernel_size=5,activation='relu')(l_embed)\n",
    "\n",
    "l_conv = Concatenate(axis=1)([l_conv_3, l_conv_4, l_conv_5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Followed by the rest of the model (boring!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_pool = MaxPooling1D(4)(l_conv)\n",
    "l_drop = Dropout(0.5)(l_pool)\n",
    "l_flat = Flatten()(l_drop)\n",
    "l_dense = Dense(32, activation='relu')(l_flat)\n",
    "preds = Dense(2, activation='softmax')(l_dense) #follows the number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile the model into a static graph for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Architecture Visualisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Embeddings Visualisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm metadata.txt logs/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"metadata.txt\",\"w\") \n",
    "\n",
    "file.write(\"<UNK>\\n\")\n",
    "\n",
    "for key in tqdm_notebook(tokenizer.word_index.items()):\n",
    "    #print(key[0], end=\" \")\n",
    "    file.write(key[0]+\"\\n\")\n",
    "    \n",
    "file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard callback\n",
    "from keras import callbacks\n",
    "tb = keras.callbacks.TensorBoard(log_dir='./logs',write_graph=True,\n",
    "                                 embeddings_freq=1, embeddings_layer_names=[],\n",
    "                                 embeddings_metadata=\"../metadata.txt\", embeddings_data=x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Progress:\")\n",
    "model_log = model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "                      epochs=1, batch_size=64, callbacks=[tb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "#model.save(\"best_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(model_log.history['acc'])\n",
    "plt.plot(model_log.history['val_acc'])\n",
    "plt.title('Accuracy (Higher Better)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(model_log.history['loss'])\n",
    "plt.plot(model_log.history['val_loss'])\n",
    "plt.title('Loss (Lower Better)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools, pickle\n",
    "\n",
    "classes = [\"positive\", \"negative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = np.argmax(y_val, axis=1) # Convert one-hot to index\n",
    "y_pred = model.predict(x_val)\n",
    "y_pred_class = np.argmax(y_pred,axis=1)\n",
    "print(classification_report(Y_test, y_pred_class, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-dark')\n",
    "def plot_confusion_matrix(cm, labels,\n",
    "                          normalize=True,\n",
    "                          title='Confusion Matrix (Validation Set)',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        #print('Confusion matrix, without normalization')\n",
    "        pass\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "cnf_matrix = confusion_matrix(Y_test, y_pred_class)\n",
    "cnf_matrix = confusion_matrix(Y_test, y_pred_class)\n",
    "plot_confusion_matrix(cnf_matrix, labels=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"cnn.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
