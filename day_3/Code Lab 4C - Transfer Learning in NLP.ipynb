{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Lab 4C - Transfer Learning in NLP\n",
    "\n",
    "## Fine-tuning ELMo for Text Classification\n",
    "\n",
    "In this Code Lab, we are going to make use of a pre-trained ELMo model from [TensorFlow Hub](https://www.tensorflow.org/hub/). ELMo is a model that makes use of a **language model** (a more complex representation compared to word embeddings) to achieve state of the art results (until recently, but that's how fast things are moving.\n",
    "\n",
    "**More on ELMo**\n",
    "\n",
    "* https://allennlp.org/elmo\n",
    "* [ArXiv: Deep contextualized word representations](https://arxiv.org/abs/1802.05365)\n",
    "\n",
    "This notebook consist of 4 main sections:\n",
    "\n",
    "1. Preparing the data\n",
    "2. Implementing a simple CNN model\n",
    "3. Training the model\n",
    "4. Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Model Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 100000    # max no. of words for tokenizer\n",
    "MAX_SEQUENCE_LENGTH = 20 # max length of each entry (sentence), including padding\n",
    "VALIDATION_SPLIT = 0.3   # data for validation (not used in training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re, sys, csv, pickle\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import keras\n",
    "from keras import regularizers, initializers, optimizers, callbacks\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing Step**\n",
    "\n",
    "Removing [stopwords](https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html), punctuation and making everything lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def clean_text(text, remove_stopwords=True):\n",
    "    output = \"\"\n",
    "    text = str(text).replace(\"\\n\", \"\")\n",
    "    text = re.sub(r'[^\\w\\s]','',text).lower()\n",
    "    if remove_stopwords:\n",
    "        text = text.split(\" \")\n",
    "        for word in text:\n",
    "            if word not in stopwords.words(\"english\"):\n",
    "                output = output + \" \" + word\n",
    "    else:\n",
    "        output = text\n",
    "    return str(output.strip())[1:-3].replace(\"  \", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading from Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = \"https://s3-ap-southeast-1.amazonaws.com/deeplearning-mat/stanford_movie.zip\"\n",
    "DATA_DIR = keras.utils.get_file(\"stanford_movie.zip\", DATA_URL, cache_subdir='datasets', extract=True)\n",
    "print(\"Dataset present at\", DATA_DIR)\n",
    "DATA_DIR = DATA_DIR.replace(\".zip\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []   # empty list for model input: the movie reviews\n",
    "labels = []  # empty lists model output: sentiment labels\n",
    "\n",
    "data_neg = open(DATA_DIR+\"/stanford_movie_neg.txt\", \"rb\") \n",
    "for line in tqdm_notebook(data_neg, total=5331): \n",
    "    texts.append(clean_text(line, remove_stopwords=False))\n",
    "    labels.append(int(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = open(DATA_DIR+\"/stanford_movie_pos.txt\", \"rb\") \n",
    "for line in tqdm_notebook(data_pos, total=5331): \n",
    "    texts.append(clean_text(line, remove_stopwords=False))\n",
    "    labels.append(int(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample negative:\", texts[0], labels[0])\n",
    "print(\"Sample positive:\", texts[9000], labels[9000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate the array of sentences from dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = [' '.join(t.split()[0:150]) for t in texts]\n",
    "all_text = np.array(all_text, dtype=object)[:, np.newaxis]\n",
    "\n",
    "labels = to_categorical(np.asarray(labels)) # convert the category label to one-hot encoding\n",
    "print('[i] Shape of data tensor:', all_text.shape)\n",
    "print('[i] Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(all_text.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "all_text = all_text[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * all_text.shape[0])\n",
    "x_train = all_text[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = all_text[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('[i] Number of entries in each category:')\n",
    "print(\"[+] Training:\",y_train.sum(axis=0))\n",
    "print(\"[+] Validation:\",y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does the data look like?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sentence input\", all_text[0])\n",
    "print(\"\")\n",
    "print(\"One-hot label\", labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create the model\n",
    "\n",
    "We will now start to create the model in `Keras`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the `ELMo` layer**\n",
    "\n",
    "Computes deep contextualized word representations using character-based word representations and bidirectional LSTMs. Paper: [Deep contextualized word representations](https://arxiv.org/abs/1802.05365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=False)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())\n",
    "\n",
    "def ElmoEmbedding(x):\n",
    "    return elmo_model(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rest of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(1,), dtype=tf.string)\n",
    "embedded_sequences = Lambda(ElmoEmbedding, output_shape=(1024,))(sequence_input)\n",
    "embedded_sequences = Reshape((1024, 1,))(embedded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_drop = Dropout(0.5)(embedded_sequences)\n",
    "l_flat = Flatten()(l_drop)\n",
    "l_dense = Dense(32, activation='relu')(l_flat)\n",
    "preds = Dense(2, activation='softmax')(l_dense) #follows the number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile the model into a static graph for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Training Progress:\")\n",
    "model_log = model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "                               epochs=15, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "plt.plot(model_log.history['acc'])\n",
    "plt.plot(model_log.history['val_acc'])\n",
    "plt.title('Accuracy (Higher Better)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(model_log.history['loss'])\n",
    "plt.plot(model_log.history['val_loss'])\n",
    "plt.title('Loss (Lower Better)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "\n",
    "classes = [\"positive\", \"negative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = np.argmax(y_val, axis=1) # Convert one-hot to index\n",
    "y_pred = model.predict(x_val)\n",
    "y_pred_class = np.argmax(y_pred,axis=1)\n",
    "print(classification_report(Y_test, y_pred_class, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-dark')\n",
    "def plot_confusion_matrix(cm, labels,\n",
    "                          normalize=True,\n",
    "                          title='Confusion Matrix (Validation Set)',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        #print('Confusion matrix, without normalization')\n",
    "        pass\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "cnf_matrix = confusion_matrix(Y_test, y_pred_class)\n",
    "plot_confusion_matrix(cnf_matrix, labels=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
