{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Lab 3: Image Classification\n",
    "\n",
    "## 1. Data Preparation\n",
    "\n",
    "#### Part A: Load the Data into Python\n",
    "\n",
    "The labels for each image are stored in a MatLab file (`imagelabels.mat`) and the images are provided to you in the folder `102flowers`. To begin, your first task will be to load everything into Python!\n",
    "\n",
    "Objective 1:\n",
    "\n",
    "* Get a list of all the labels\n",
    "* Get a list of all the images\n",
    "* Check that the length of both lists are the same!\n",
    "\n",
    "Hint: look at the imports!\n",
    "\n",
    "Note that not everything returned by loadmat(...) is useful, as the returned dictionary even contains certain metadata about the file. You should just pick out what you want, which in this case, is the array of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Objective 1:\n",
    "# Load labels from .mat (MatLab) file into a Python list\n",
    "# Hint: always look at the imports given to you!\n",
    "\n",
    "# answer\n",
    "\n",
    "# end answer\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code block below to check if you've made any error. If you get an AssertionError, it's most likely you've forgotten that __we want the labels to be a list, not a list of lists__. If the code runs without any error, move on. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(labels) == 8189"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.utils import get_file\n",
    "\n",
    "# download the dataset\n",
    "\n",
    "DATASET_URL = \"https://s3-ap-southeast-1.amazonaws.com/deeplearning-iap-material/102flowers.zip\"\n",
    "DATASET_DIR = keras.utils.get_file(\"102flowers.zip\", DATASET_URL, cache_subdir='datasets', extract=True)\n",
    "DATASET_DIR = DATASET_DIR.replace(\".zip\",\"\")+\"/102flowers\"\n",
    "print(\"Dataset downloaded to:\", DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the bunch of jpgs!\n",
    "!ls /home/jovyan/.keras/datasets/102flowers/102flowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the directory where the images are found\n",
    "DATASET_DIR = \"/home/jovyan/.keras/datasets/102flowers/102flowers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll be using glob, which is a library that helps us look for pathnames that match a certain pattern that you specify. Take a minute to refer to the [documentation here](https://docs.python.org/2/library/glob.html) to familiarise yourself with glob. We'll be using it quite a bit in our codelabs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Objective 1:\n",
    "# Get a list of images in the folder \"102flowers\"\n",
    "\n",
    "# answer\n",
    "\n",
    "# end answer\n",
    "\n",
    "# check answer\n",
    "print(\"Path to first image:\", image_paths[0]) # ./102flowers/image_00001.jpg\n",
    "print(\"Path to last image:\", image_paths[-1]) # ./102flowers/image_08189.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected Output\n",
    "\n",
    "```\n",
    "Path to first image: /home/jovyan/.keras/datasets/102flowers/102flowers/image_00001.jpg\n",
    "Path to last image: /home/jovyan/.keras/datasets/102flowers/102flowers/image_08189.jpg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that number of images and labels correspond\n",
    "# Hint: use assert\n",
    "\n",
    "# answer\n",
    "\n",
    "# end answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B: Create train-val split\n",
    "\n",
    "We need to split our image dataset into a training and a validations set. Ideally, there should be a third test set. However, we are not going to do that today in the interest of keeping the Code Lab more streamlined. \n",
    "\n",
    "Note: depending on the library, words like \"test\", \"validation\" and \"evaluation\" may be interchangable.\n",
    "\n",
    "Objective 2:\n",
    "\n",
    "* split the images into 70% training and 30% validation set\n",
    "  \n",
    "  (hint: `test_size=0.3`)\n",
    "  \n",
    "You may find [this documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Objective 2:\n",
    "\n",
    "# answer\n",
    "\n",
    "# end answer\n",
    "\n",
    "total_train = len(images_train)\n",
    "total_val = len(images_val)\n",
    "\n",
    "print(\"Training set:\", total_train)\n",
    "print(\"Validation set:\", total_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to check your percentage of validation data\n",
    "print(\"Percent validation data\", round(total_val/(total_train+total_val)*100,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C: Pre-process and organise into folders (important for keras data_generator!)\n",
    "\n",
    "Objective 3:\n",
    "\n",
    "* preprocess the images by resizing (our CNN will take in images that are 299 &times; 299).\n",
    "\n",
    "  For this, make use of the provided `resize_and_pad` function to complete the `preprocess_image` function. We will again use `preprocess_image` later in the Code Lab.\n",
    "\n",
    "\n",
    "* The padding for the image can be a colour of your choosing (default is black == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper code\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def make_folder(path):\n",
    "    \"\"\"\n",
    "    Create a directory (folder) unless it already exists.\n",
    "    \"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "def resize_and_pad(img, size, pad_colour=0):\n",
    "    \"\"\"\n",
    "    Resizes image, maintaining aspect ratio and filling in the excess areas with pad_colour\n",
    "    \"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    sh, sw = size\n",
    "    # interpolation method\n",
    "    if h > sh or w > sw:\n",
    "        # shrinking image\n",
    "        interp = cv2.INTER_AREA\n",
    "    else:\n",
    "        # stretching image\n",
    "        interp = cv2.INTER_CUBIC\n",
    "\n",
    "    aspect = w/h\n",
    "\n",
    "    if aspect > 1:\n",
    "        # horizontal image\n",
    "        new_w = sw\n",
    "        new_h = np.round(new_w/aspect).astype(int)\n",
    "        pad_vert = (sh-new_h)/2\n",
    "        pad_top, pad_bot = np.floor(pad_vert).astype(int), np.ceil(pad_vert).astype(int)\n",
    "        pad_left, pad_right = 0, 0\n",
    "    elif aspect < 1:\n",
    "        # vertical image\n",
    "        new_h = sh\n",
    "        new_w = np.round(new_h*aspect).astype(int)\n",
    "        pad_horz = (sw-new_w)/2\n",
    "        pad_left, pad_right = np.floor(pad_horz).astype(int), np.ceil(pad_horz).astype(int)\n",
    "        pad_top, pad_bot = 0, 0\n",
    "    else:\n",
    "        # square image\n",
    "        new_h, new_w = sh, sw\n",
    "        pad_left, pad_right, pad_top, pad_bot = 0, 0, 0, 0\n",
    "\n",
    "    if len(img.shape) is 3 and not isinstance(pad_colour, (list, tuple, np.ndarray)):\n",
    "        # color image but only one color provided\n",
    "        pad_colour = [pad_colour]*3\n",
    "\n",
    "    scaled_img = cv2.resize(img, (new_w, new_h), interpolation=interp)\n",
    "    scaled_img = cv2.copyMakeBorder(scaled_img, pad_top, pad_bot, pad_left, pad_right,\n",
    "                                    borderType=cv2.BORDER_CONSTANT, value=pad_colour)\n",
    "\n",
    "    return scaled_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    # Objective 3:\n",
    "    # Resize and pad the images\n",
    "    \n",
    "    # answer\n",
    "    \n",
    "    # end answer\n",
    "    \n",
    "    return processed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now begin preprocessing and writing our training/validation images into folders, where every folder will contain images of the certain class. We use the `tqdm` module to show our progress in a nice way. In the interest of time, we will ignore all classes above a certain number (index). This is set using the `IGNORE_ABOVE` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_ABOVE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r ./train ./val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_DIR = \"./train/\"\n",
    "\n",
    "for image_path in tqdm_notebook(images_train):\n",
    "    try:\n",
    "        # get image index\n",
    "        image_index = int(image_path.split(\"_\")[1].replace(\".jpg\",\"\"))\n",
    "        # get class index\n",
    "        class_index = labels[image_index-1]\n",
    "        \n",
    "        if int(class_index)>IGNORE_ABOVE:\n",
    "            pass\n",
    "        else:\n",
    "            class_folder = TRAIN_DATA_DIR+str(class_index)+\"/\"\n",
    "\n",
    "            image = cv2.imread(image_path)\n",
    "            image = preprocess_image(image)\n",
    "\n",
    "            # make class folder\n",
    "            make_folder(class_folder)\n",
    "\n",
    "            # make image output path and write image\n",
    "            image_output_path = class_folder+str(image_index)+\".jpg\"\n",
    "\n",
    "            cv2.imwrite(image_output_path, image)\n",
    "    except Exception as e:\n",
    "        print(image_path, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_DATA_DIR = \"./val/\"\n",
    "\n",
    "for image_path in tqdm_notebook(images_val):\n",
    "    try:\n",
    "        # get image index\n",
    "        image_index = int(image_path.split(\"_\")[1].replace(\".jpg\",\"\"))\n",
    "        # get class index\n",
    "        class_index = labels[image_index-1]\n",
    "        if int(class_index)>IGNORE_ABOVE:\n",
    "            pass\n",
    "        else:\n",
    "            class_folder = VAL_DATA_DIR+str(class_index)+\"/\"\n",
    "\n",
    "            image = cv2.imread(image_path)\n",
    "            image = preprocess_image(image)\n",
    "\n",
    "            # make class folder\n",
    "            make_folder(class_folder)\n",
    "\n",
    "            # make image output path and write image\n",
    "            image_output_path = class_folder+str(image_index)+\".jpg\"\n",
    "\n",
    "            cv2.imwrite(image_output_path, image)\n",
    "    except Exception as e:\n",
    "        print(image_path, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part D: Sanity check!\n",
    "\n",
    "Let's check our directories and images and see if they turn out the way we expect.\n",
    "\n",
    "**Important**:  pay attention to check out how the folders are ordered..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train folder contains:\")\n",
    "!ls train\n",
    "print(\"\\ntrain/1 folder contains:\")\n",
    "!ls train/1\n",
    "\n",
    "print(\"\\nval folder contains:\")\n",
    "!ls val\n",
    "print(\"\\nval/1 folder contains:\")\n",
    "!ls val/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Displaying an image**\n",
    "\n",
    "Now, we want to load an image to check if our preprocessing is appropriate. OpenCV loads images in BGR, while matplotlib uses RGB. Hence, we need to correct the **colour space** of the image if we want to see it properly in matplotlib.\n",
    "\n",
    "Documentation: https://docs.opencv.org/3.2.0/df/d9d/tutorial_py_colorspaces.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(image_output_path)\n",
    "\n",
    "image = cv2.imread(image_output_path) # let's load the last image we wrote\n",
    "\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Keras Model\n",
    "\n",
    "Now, we will build our input pipeline, the Keras model and then begin our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "from keras import optimizers, callbacks, regularizers\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.xception import preprocess_input\n",
    "\n",
    "# some parameters we want to decide beforehand\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "INPUT_SIZE = (299,299)\n",
    "NUM_CLASS = len(glob.glob(\"./val/*\")) # very useful to have!\n",
    "print(\"Number of classes:\", NUM_CLASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A: Input Pipeline\n",
    "\n",
    "We will create our ImageDataGenerator here. Keras's ImageDataGenerator allows us to _perform data augmentation on-the-fly_, and allows us to use the very useful __fit_generator__ function later on when training. \n",
    "\n",
    "Note that the ImageDataGenerator __automatically infers the number of classes you have based on your directory structure__ i.e. If you have 3 folders, Keras will naturally assume that each folder is a separate class by itself, and the images within the folder are images specific to that class.\n",
    "\n",
    "You can perform the data augmentations by specifying the arguments in the ImageDataGenerator. You can look up the available augmentations here: https://keras.io/preprocessing/image/ .There are really no hard-and-fast rules to what sort of augmentations you can or should apply, so it's a matter of testing to see which ones work best for your task at hand.\n",
    "\n",
    "Finally, we create separate ImageDataGenerators for our train, validation, and test datasets. In this case, we only have a train and validation dataset, so we'll only be creating two IDGs. Note that we need not apply the same augmentation to both the training and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Train Data Generator\")\n",
    "train_datagen = ImageDataGenerator(rotation_range=90,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   preprocessing_function=preprocess_input,\n",
    "                                   fill_mode='nearest')\n",
    "train_generator = train_datagen.flow_from_directory('./train/',\n",
    "                                                    target_size=INPUT_SIZE,\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "print(\"\\nCreating Validation Data Generator\")\n",
    "validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "validation_generator = validation_datagen.flow_from_directory('./val/',\n",
    "                                                              target_size=INPUT_SIZE,\n",
    "                                                              batch_size=BATCH_SIZE,\n",
    "                                                              class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part B: Build the model\n",
    "\n",
    "We are using transfer learning by applying a pre-trained [Xception (Chollet, 2017)](https://arxiv.org/abs/1610.02357) CNN and fine-tuning a final classification layer. If you want, you can also try using a pre-trained ResNet50 instead, and see which model performs better.\n",
    "\n",
    "To build the model, we will be using the [Keras Model (Functional) API](https://keras.io/getting-started/functional-api-guide/). This API gives us flexibility and allows us to specify how we want the tensors to flow through our model. To do this, we need to do 4 things.\n",
    "1. Specify your Input tensor\n",
    "2. Instantiate your base model (Xception or ResNet50)\n",
    "3. Add the last few layers to suit your classification task\n",
    "4. Instantiate your Model instance and compile it\n",
    "\n",
    "***\n",
    "__Step 1__  \n",
    "In the first step, we need to specify the very first input tensor for Keras, and then Keras will automatically infer the subsequent tensor shapes for us, so we don't need to bother about keeping track of the changes in tensor shapes after the first layer. We specify the input tensor by passing the input shape as arguments as such: Input(_width_, _height_, _depth_). Hence, if your images are of size 50 x 50 x 3 for example, you would specify Input(shape=(50,50,3)).\n",
    "\n",
    "Those of you who are sharp might notice that since we are sending in batches of data, the input shape should actually be a 4D tensor i.e. (batches, width, height, depth), and yet we are only specifying 3 arguments. This is because the Model API has been designed such that we _do not_ specify the batch size when passing the arguments in Input. Keras will figure that out by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(INPUT_SIZE[0], INPUT_SIZE[1], 3)) # input image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2__  \n",
    "In the second step, we need to instantiate our base model. For this, you can choose to use either Xception or ResNet50. Xception is a newer model developed by the creator of Keras himself, Francois Chollet. However, in practice ResNet is the more popular and widely-used model. It is used as the model backbone to architectures like Mask RCNN, which is used in object detection tasks.\n",
    "\n",
    "Check that you've imported Xception (or ResNet50) from `keras.applications.xception` (or `keras.applications.resnet50`). You should have done so among the other import statements in one of the code blocks above. Now, in order to instantiate your base model, you will need to specify 3 arguments:\n",
    "1. input_tensor\n",
    "2. weights\n",
    "3. include_top\n",
    "\n",
    "Refer to the documentation [here](https://keras.io/applications/#xception) to get clues on what the arguments should be. (Hints: Remember that we are applying __transfer learning__ - refer to the slides if you've forgotten what needs to be done to apply transfer learning properly). Note that if this is the first time you are downloading the base model, it might take some time to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building base model for Xception...\")\n",
    "\n",
    "# start answer\n",
    "#base = Xception(...)\n",
    "\n",
    "# end answer\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "# You can also use ResNet50.\n",
    "# If you wish to do so, comment out the 3 lines above and carry on \n",
    "# with the helper code below.\n",
    "\n",
    "\"\"\"\n",
    "print(\"Building base model for Resnet50...\")\n",
    "base = ResNet50(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
    "print(\"Done!\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3__  \n",
    "Next, we need to add in our own layers to the base model to suit our task at hand. To do this, you must understand how the Model(Functional) API works, and you will need to be familiar with Keras layers. Take a few minutes to read the documentation [here](https://keras.io/getting-started/functional-api-guide/#all-models-are-callable-just-like-layers), and [here](https://keras.io/layers/core/).\n",
    "\n",
    "In Keras, all models are made up of layers. The layers in Keras are objects that perform mathematical operations  that should be familiar to you by now. For instance, the Conv2D layer performs convolutions. The Pooling2D layer performs pooling. The Dense layer is a fully-connected layer, which performs matrix multiplcations followed by an activation function. These layers are configurable by passing arguments. For instance, Dense(64, activation='relu') creates a fully-connected layer with 64 neurons, with a ReLU activation function.\n",
    "\n",
    "In the Model(Functional) API, __every layer is callable on a tensor and returns a tensor__. In other words, `y = Dense(...)(x)` is basically putting an input tensor called 'x' through a Dense layer, and then returning an output tensor called 'y'. 'y' can then be used as an input tensor into another layer. For convenience, we typically just use `x = Layer(..)(x)` for all the layers except the last layer since we don't really care about the tensors in between.\n",
    "\n",
    "To do this step, you'll need to add at least 4 new layers in sequential order. \n",
    "\n",
    "1. a pooling layer of your choosing, \n",
    "2. a dense layer, \n",
    "3. a dropout layer, and then another \n",
    "4. dense layer.\n",
    "\n",
    "Note that since we are adding these layers after the base model, we will need to access the tensors coming out of the base model and use them as inputs to the first of these 4 layers. __base.output equals to the output tensor coming out of the base model__.\n",
    "\n",
    "It is up to you to specify the arguments of the 4 layers - but bear a few things in mind. For pooling, at the last few layers of a CNN, we don't often use anything larger than 2x2, and usually without any stride as well. For dense layer, the number of units will determine the shape of the output tensor. As an example, passing in a tensor with shape (batch_size, input_dim) will produce an output tensor of shape (batch_size, num_units). For dropout, anything less than 0.7 is fine.  \n",
    "\n",
    "__You'll need to be especially careful about the last Dense layer__ - think carefully about what the output of the entire CNN should be in this case. (Hint: The output of a CNN should be a probability distribution over the number of classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = GlobalAveragePooling2D()(base.output)\n",
    "x = Dense(NUM_CLASS*2, activation='relu')(features)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(NUM_CLASS, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will need to freeze the weights of the base model since they have already been pre-trained. This can be done easily by doing a for loop over the base layers, and setting layer.trainable to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perhaps in the future, we can set some layers to trainable!\n",
    "\n",
    "for layer in base.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 4__  \n",
    "Last but not least. we will need to instantiate the model, and compile it. We instantiate the model by specifying the named tensors which will be its input and output. \n",
    "\n",
    "Compiling the model is also easy - it takes just one line of code. You'll need to specify the loss function, the optimizer and the metrics to track while training the model.   \n",
    "\n",
    "- For the loss function, note that we are doing a __multi-class classification problem__ (as opposed to a binary classification problem such as classifying if an image is a dog or a cat).\n",
    "\n",
    "- For the optimizer, you can take your pick among the optimizers listed [here](https://keras.io/optimizers/) in the Keras documentation, but we recommend using Adam. You may stick to the default values for the parameters in the optimizer, but feel free to tune them and watch what happens to your training time, model accuracy, etc.\n",
    "\n",
    "- Finally, for metrics, we typically track accuracy (_note that metrics argument is expecting a list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=input_tensor, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', # choose a loss function!!\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Uncomment to take a look at your model!\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! Now you have everything ready to train the model! To do this, we'll be using the `fit_generator` function.\n",
    "\n",
    "\n",
    "First, let's just instantiate `callbacks_list` as an empty list. You don't need to worry about callbacks for now. We'll be passing this empty list as an argument later on to our fit_generator. Typically, we should specify particular callbacks rather than an empty list, but for simplicity sake we won't be \n",
    "doing so here. If you're interested though, you may want to refer to the [Keras documentation on callbacks](https://keras.io/callbacks/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of some useful callbacks\n",
    "\n",
    "\"\"\"\n",
    "from keras import callbacks\n",
    "\n",
    "tb = callbacks.TensorBoard(log_dir='./logs',write_graph=True)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n",
    "\n",
    "callbacks_list = [tb, reduce_lr]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just to get the size of our training and validation dataset. \n",
    "total_train = len(glob.glob(\"./train/*/*.jpg\"))\n",
    "total_val = len(glob.glob(\"./val/*/*.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know what arguments to specify in `fit_generator`, refer to the Keras documentation [here](https://keras.io/models/sequential/). Note that you can stick to most of the default arguments. Just remember that \n",
    "\n",
    "- The number of steps per epoch is equal to the dataset (floor) divided by the batch size\n",
    "- You want to make use of your validation data generator\n",
    "- Specify at least 20 epochs (we recommend 25, but it's up to you if you want to train for more epochs if you're willing to wait it out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_train = int(total_train/BATCH_SIZE)\n",
    "steps_val = int(total_val/BATCH_SIZE)\n",
    "\n",
    "NUM_THREADS = N # set to number of cores allocated to you\n",
    "\n",
    "print(\"Training Progress:\")\n",
    "model_log = model.fit_generator(train_generator, validation_data=validation_generator,\n",
    "                                epochs=20, workers=NUM_THREADS, use_multiprocessing=False,\n",
    "                                steps_per_epoch=steps_train, validation_steps=steps_val,\n",
    "                                callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper code to help you plot the training curves\n",
    "plt.plot(model_log.history['acc'])\n",
    "plt.plot(model_log.history['val_acc'])\n",
    "plt.title('Accuracy (Higher Better)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(model_log.history['loss'])\n",
    "plt.plot(model_log.history['val_loss'])\n",
    "plt.title('Loss (Lower Better)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation\n",
    "\n",
    "A model is not useful unless we can measure how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# learn more about sklearn.metrics:\n",
    "# https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember how the folders were ordered when you `ls`? Now we need to fix the class indexes. Print `name_id_map` if you don't understand why the ordering is messed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads all the folders in which images are present\n",
    "class_names = glob.glob(\"./val/*\")\n",
    "class_names = sorted(class_names) # Sorting them\n",
    "fixed_classes = []\n",
    "for class_name in class_names:\n",
    "    fixed_classes.append(class_name.split(\"/\")[-1])\n",
    "name_id_map = dict(zip(range(len(class_names)), fixed_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_paths = glob.glob('./val/*/*.jpg')\n",
    "print(\"Example image path:\", test_image_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can take a look at the accuracy score for the model. That is, how many images it classified correctly over the total number of images. Note a few things:\n",
    "\n",
    "- You want to apply the same preprocessing that you did earlier in order to be consistent\n",
    "- You can simply use the `model.predict(...)` function in order to pass an image through the model and get an output\n",
    "- The output of predict is a probability distribution over the classes. In order to know which class is actually predicted, you might find [np.argmax(...)](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.argmax.html) useful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ground_truth = [image_path.split(\"/\")[-2] for image_path in test_image_paths]\n",
    "test_preds = []\n",
    "\n",
    "for image_path in tqdm_notebook(test_image_paths):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = preprocess_image(image)\n",
    "    image = image.reshape((1,299,299,3)) * (1./127.5) - 1\n",
    "    preds = model.predict(image)\n",
    "    output_pred = name_id_map[np.argmax(preds,axis=1)[0]]\n",
    "    test_preds.append(output_pred)\n",
    "    \n",
    "print(\"Accuracy:\", accuracy_score(test_ground_truth, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report\")\n",
    "print(classification_report(test_ground_truth, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(test_ground_truth, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You made it!\n",
    "\n",
    "In this code lab, you learned how to:\n",
    "\n",
    "* load a \"large\" image dataset\n",
    "* preprocess and organise your dataset for Keras data_generator\n",
    "* use data_generator with image augmentation\n",
    "* train with fit_generator and fix the class indexes\n",
    "* use transfer learning with a pre-trained CNN\n",
    "* evaluate the performance of your model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
